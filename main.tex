\documentclass[12pt,twoside]{report}

% some definitions for the title page
\newcommand{\reporttitle}{A New Scalable Runtime for LLM Inference}
\newcommand{\reportauthor}{Hamish McCreanor}
\newcommand{\supervisor}{Peter Pietzuch}
\newcommand{\reporttype}{MEng Individual Project}
\newcommand{\degreetype}{MEng Computing} 

% load some definitions and default packages
\input{includes}

% load some macros
\input{notation}

% load title page
\begin{document}
\input{titlepage}


% page numbering etc.
\pagenumbering{roman}
\clearpage{\pagestyle{empty}\cleardoublepage}
\setcounter{page}{1}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
\begin{abstract}
Your abstract.
\end{abstract}
\end{comment}

\cleardoublepage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
\section*{Acknowledgments}
Comment this out if not needed.

%\clearpage{\pagestyle{empty}\cleardoublepage}
\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--- table of contents
\fancyhead[RE,LO]{\sffamily {Table of Contents}}
\tableofcontents 


%\clearpage{\pagestyle{empty}\cleardoublepage}
\pagenumbering{arabic}
\setcounter{page}{1}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction} % 1-3 pages
% What is the problem?
% Why is it interesting?
% How do I propose to solve it?
As large language models (LLMs) are found useful for ever-wider classes of applications, a trend has arisen focusing on the low-cost, local deployment of these systems.
While the training of LLMs like LLaMA, BERT and OpenAI's GPTs typically requires months of training and is prohibitive for all but the most well-funded of organisations, performing inference on these models locally is comparatively more feasible.
This enables developers to create services with tighter LLM integrations - instead of calling a black-box API provided by an LLM provider, they can instead run a local version of the LLM, tuning the inference runtime to more appropriately match the context in which it is called.

As a result, there is currently a vast body of research aiming to improve existing inference systems. 
The aim of this is to improve LLM inference performance along various axes.
These include running on lower-powered hardware; running with improved throughput and running at greater energy efficiencies.
These optimisations focus on specific elements of the inference pipeline, particularly improving KV cache usage and kernel fusion.
To build systems containing these optimisations, developers frequently turn to high level languages like Python in order to quickly develop the infrastructure surrounding their new technique.
Developing this way limits the ability of the system to exploit memory-access patterns and application parallelism (especially in a language like Python, with its global interpreter lock) and incurs unnecessary overhead.

This project aims to build on the existing llama.cpp inference server (see \ref{section:llamacpp}) to deliver a system that improves the dispatch of compute kernels by better parallelising the inference pipeline.
% Talk more about how I will build on llama.cpp

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background} % 10-20 pages
We provide an overview of transformer architecture in \ref{subsection:llmsexplained}, as well as key components of the inference pipeline in \ref{subsection:llminference}, before detailing existing work around LLM inference runtimes in \ref{section:relatedwork}.
\section{Preliminaries}
\subsection{LLMs Explained}\label{subsection:llmsexplained}
\subsubsection{Transformer Architecture}
While the term ``Large Language Model'' can be used to describe any model trained on large volumes of textual data, it is frequently used to refer to models that use a variant of the transformer architecture described in \cite{vaswani2017attention}.
This architecture has superseded recurrent neural networks for language-based tasks owing to its ability to capture long range dependencies between tokens.
It does this via a unique attention mechanism.
This, and the other components present in a transformer, are described below.
\begin{itemize}
  \item \textbf{Embedding Layers:} 
    Attention blocks are fed a matrix where each row represents the semantic meaning of each token. This is done by using a learned embedding and applying this to the input tokens.
  \item \textbf{Positional Encoding:} 
    Unlike recurrent neural networks, transformer models contain no a priori knowledge of the order of the input sequence.
    To remedy this, positional encodings are added to the input embeddings before they are fed to the attention block.
    There are several ways of doing this, however the approach adopted in the \cite{vaswani2017attention} is to use apply sine and cosine functions to the position and dimension of the input vector as follows:
    \begin{equation*}
      \begin{split}
        \textnormal{PE}_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}) \\ 
        \textnormal{PE}_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})
      \end{split}
    \end{equation*}
  \item \textbf{Multi-Head Attention:}
    Transformers use an attention block to describe the relationship between different tokens in a sequence. 
    This takes as input a key, query and value matrix $K$, $Q$, $V$ respectively. 
    The key and query matrices are multiplied together to produce a representation of how each token in the sequence relates to each other token.
    After normalising and applying a softmax function, the matrix is multiplied by $V$, which represents the semantic meaning of each input token.
    \begin{equation*}
      \textnormal{Attention}(Q,K,V) = \textnormal{softmax}(\frac{QK^T}{\sqrt{d_k}})V
    \end{equation*}
    In practise, multiple attention heads are used and their outputs concatenated.  
    This allows the model to attend to information learned from different projections $W_i^Q$, $W_i^K$ and $W_i^V$ at the same time.  
    \begin{equation*}
      \begin{split}
        \textnormal{MultiHead}(Q,K,V) = \textnormal{Concat}(\textnormal{head}_1, ..., \textnormal{head}_h)W^O \\
        \textnormal{where} \: \textnormal{head}_i = \textnormal{Attention}(QW_i^Q, KW^K_i, VW^V_i)
      \end{split}
    \end{equation*}
  \item \textbf{Position-wise Fead-Forward Networks:} 
    The output of the multi-head attention block is fed to a fully connected neural-network.
    This takes as input the representation at each position (making it ``position-wise'') and applies two linear transformations separated by a ReLU activation:
    \begin{equation*}
      \textnormal{FFN}(x) = \textnormal{max}(0, xW_1 + b_1)W_2 + b_2
    \end{equation*}
\end{itemize}
Multiple blocks consisting of the multi-head attention and feed-forward layers are stacked on top of one another to produce a deeper transformer model.
\subsubsection{Model Variants}
The encoder-decoder model introduced in \cite{vaswani2017attention} is not the only variant of the transformer architecture that exists.
Modifications to the transformer architecture have been made in order to better support different tasks:  
\begin{itemize}
  \item \textbf{Encoder-decoder:}
    With encoder-decoder models, an input sequence is first embedded, then added to a positional encoding before being passed as input to a stack of attention and feed-forward network layers (the ``encoder'') to produce a representation of the input sequence. 
    As the output sequence is generated, it is fed as input to a similarly structured ``decoder'' block. 
    This takes an embedded output sequence, with positional encoding, as input.
    The output of the encoder block is fed to attention blocks in the decoder block in order to model the relationship between the input and output sequence.
    These models \cite{raffel2020exploring} tend to be used for tasks like sequence to sequence transformation, for example language translation.
  \item \textbf{Encoder-only:}
    Encoder-only models use only the encoder block of an encoder-decoder model to produce a vector representation of an input sequence.
    These models do not produce text directly, but are instead meant to pass their output to a downstream component for further inference, potentially for applications to sentiment analysis or named-entity recognition.
    Examples of these include the BERT \cite{kenton2019bert} family of models.
  \item \textbf{Decoder-only:}
    Decoder-only models like those in the GPT \cite{radford2018improving} series make up the bulk of models used for auto-regressive text generation and completion. 
    Instead of sending an input prompt to an encoder block, the prompt is instead passed in in conjuction to the output sequence and the model is trained to predict the next token in the sequence \cite{dai2015semi}.
\end{itemize}



\subsection{LLM Inference}\label{subsection:llminference}
\subsubsection{KV Cache}
\subsubsection{Parallelism}
\begin{comment}
(sources)
https://docs.vllm.ai/en/latest/serving/distributed_serving.html
https://colossalai.org/docs/concepts/paradigms_of_parallelism/


data parallel
- copy model to different gpus
	- can be used to train on multiple batches in parallel

tensor parallel 
- split the tensors for a single layer across gpus

pipeline parallel
- split whole layers across gpus
\end{comment}

\subsubsection{Request Batching}

\section{Related Work}\label{section:relatedwork}
\subsection{vLLM}
\subsubsection{PagedAttention}
\subsection{Triton}
\subsection{SGLang}
\subsection{Triton}
\subsection{llama.cpp} \label{section:llamacpp}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Project Plan} % 1-2 pages
% Break down runtime implementation (this probably requires a bit of knowledge about how llama.cpp works)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Evaluation Plan} % 1-2 pages
\begin{comment}
1. Run some benchmark on llama.cpp
2. Run same benchmark on llama.cpp + my system
3. ...
4. Profit???

Key requirements for project

Areas on which I could track performance relative to existing system
- Efficiency (\% of overall resources used)
- Throughput (tokens/sec)
\end{comment}

\section{Functional Requirements}
\section{Performance Metrics}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Ethical Issues} % 1-2 pages
% Explain all the bad things llms can do
% Two angles
% Misuse and data protection
% Explain how this project can contribute
% Greater availability of llms enables smaller actors to misuse them
The principal two ethical concerns of a project in this field relate to the potential for misuse as well as provenance issues surrounding the dataset on which the model was trained.

The potential for misuse of LLMs is vast, with many instances of LLM abuse already being documented.
LLM abuse typically involves the use of the model to produce harmful or misleading content.
There already exist proof-of-concepts for LLMs \cite{hazell2023spear} being used to generate phishing messages, with the intent to produce emails that sound more plausible and are more likely to be engaged with by a target.
In addition to this, LLMs can be used to produce vast quantities misinformation or biased content that are then published to social media platforms \cite{williams2024large}. 
End users may be unable to distinguish between content created by a genuine user and content generated by an LLM and thus end up misinformed.

The large size of the datasets required to train these models create potential ethical and data protection issues.
Concerns exist regarding the ability for generative models to amplify existing biases in their training data, with some of these concerns borne out in cases like Microsoft's Tay chatbot \cite{tayMicrosoft}.
AI fairness is still an open area of research \cite{xivuri2021systematic} and it is unlikely that existing LLM models will be completely free of bias at inference time.
At the same time, the provenance of this training data is also an important ethical consideration.
Private or sensitive data has the potential to be incorporated into training sets and there exist cases \cite{nasr2023scalable} where this training data has then been generated verbatim at inference time, exposing this sensitive data to an end user.

If successful, our project broadens access to LLMs by making better use of available hardware to perform inference.
This increases the viability of local inference and opens up these models to a greater proportion of hardware configurations and thus a greater number of users.
While this represents a boon for the accessibility of this technology, with users no longer limited to a handful of offerings by large companies, it also increases the number of potentially malicious actors who are able to use LLMs.
Small and local deployments likely have less of the oversight that large LLM providers experience, and thus are more able to misuse this technology.
These two elements must be carefully managed in order to produce a project that adheres to reasonable ethical standards.

As this project represents a proof-of-concept, rather than a full-featured inference engine, any advances made are unlikely to immediately be adopted and thus any ethical concerns are likely to be uncovered at a pace with which they can be identified early on and mitigated quickly.


%% bibliography
\bibliographystyle{vancouver}
\bibliography{references}


\end{document}
