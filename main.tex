\documentclass[12pt,twoside]{report}

% some definitions for the title page
\newcommand{\reporttitle}{A New Scalable Runtime for LLM Inference}
\newcommand{\reportauthor}{Hamish McCreanor}
\newcommand{\supervisor}{Peter Pietzuch}
\newcommand{\reporttype}{MEng Individual Project}
\newcommand{\degreetype}{MEng Computing} 

% load some definitions and default packages
\input{includes}

% load some macros
\input{notation}

% load title page
\begin{document}
\input{titlepage}


% page numbering etc.
\pagenumbering{roman}
\clearpage{\pagestyle{empty}\cleardoublepage}
\setcounter{page}{1}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
\begin{abstract}
Your abstract.
\end{abstract}
\end{comment}

\cleardoublepage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
\section*{Acknowledgments}
Comment this out if not needed.

%\clearpage{\pagestyle{empty}\cleardoublepage}
\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--- table of contents
\fancyhead[RE,LO]{\sffamily {Table of Contents}}
\tableofcontents 


%\clearpage{\pagestyle{empty}\cleardoublepage}
\pagenumbering{arabic}
\setcounter{page}{1}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction} % 1-3 pages
% What is the problem?
% Why is it interesting?
% How do I propose to solve it?
As large language models (LLMs) are found useful for ever-wider classes of applications, a trend has arisen focusing on the low-cost, local deployment of these systems.
While the training of LLMs like LLaMA, BERT and OpenAI's GPTs typically requires months of training and is prohibitive for all but the most well-funded of organisations, performing inference on these models locally is comparatively more feasible.
This enables developers to create services with tighter LLM integrations - instead of calling a black-box API provided by an LLM provider, they can instead run a local version of the LLM, tuning the inference runtime to more appropriately match the context in which it is called.

As a result, there is currently a vast body of research aiming to improve existing inference systems. 
The aim of this is to improve LLM inference performance along various axes.
These include running on lower-powered hardware; running with improved throughput and running at greater energy efficiencies.
These optimisations focus on specific elements of the inference pipeline, particularly improving KV cache usage and kernel fusion.
To build systems containing these optimisations, developers frequently turn to high level languages like Python in order to quickly develop the infrastructure surrounding their new technique.
Developing this way limits the ability of the system to exploit memory-access patterns and application parallelism (especially in a language like Python, with its global interpreter lock) and incurs unnecessary overhead.

This project aims to build on the existing llama.cpp inference server (see \ref{section:llamacpp}) to deliver a system that improves the dispatch of compute kernels by better parallelising the inference pipeline.
% Talk more about how I will build on llama.cpp

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background} % 10-20 pages
We provide an overview of transformer architecture in \ref{subsection:llmarchitecture}, as well as key components of the inference pipeline in \ref{subsection:llminference}, before detailing existing work around LLM inference runtimes in \ref{section:relatedwork}.
\section{Preliminaries}
\subsection{LLM Architecture}\label{subsection:llmarchitecture}
\subsubsection{Transformer Architecture}
While the term ``Large Language Model'' can be used to describe any model trained on large volumes of textual data, it is frequently used to refer to models that use a variant of the transformer architecture described in \cite{vaswani2017attention}.
This architecture has superseded recurrent neural networks for language-based tasks owing to its ability to capture long range dependencies between tokens.
It does this via a unique attention mechanism.
This, and the other components present in a transformer, are described below.
\begin{itemize}
  \item \textbf{Embedding Layers:} 
    Attention blocks are fed a matrix where each row represents the semantic meaning of each token. This is done by using an embedding learnt during the training process and applying this to the input tokens.
  \item \textbf{Positional Encoding:} 
    Unlike recurrent neural networks, transformer models contain no a priori knowledge of the order of the input sequence.
    To remedy this, positional encodings are added to the input embeddings before they are fed to the attention block.
    There are several ways of doing this, however the approach adopted in the \cite{vaswani2017attention} is to use apply sine and cosine functions to the position and dimension of the input vector as follows:
    \begin{equation*}
      \begin{split}
        \textnormal{PE}_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}) \\ 
        \textnormal{PE}_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})
      \end{split}
    \end{equation*}
  \item \textbf{Multi-Head Attention:}
    Transformers use an attention block to describe the relationship between different tokens in a sequence. 
    This takes as input a key, query and value matrix $K$, $Q$, $V$ respectively. 
    The key and query matrices are multiplied together to produce a representation of how each token in the sequence relates to each other token.
    After normalising and applying a softmax function, the matrix is multiplied by $V$, which represents the semantic meaning of each input token.
    \begin{equation*}
      \textnormal{Attention}(Q,K,V) = \textnormal{softmax}(\frac{QK^T}{\sqrt{d_k}})V
    \end{equation*}
    In practise, multiple attention heads are used and their outputs concatenated.  
    Instead of attention blocks just operating on the $K$, $Q$, and $V$ directly, they are multiplied by learned matrices $W^K$, $W^Q$, and $W^V$ to project the input vectors into different spaces.
    The fact that there are multiple of these matrices $W_i^Q$, $W_i^K$ and $W_i^V$ allows the model to attend to information learnt from different projections concurrently.
    \begin{equation*}
      \begin{split}
        \textnormal{MultiHead}(Q,K,V) = \textnormal{Concat}(\textnormal{head}_1, ..., \textnormal{head}_h)W^O \\
        \textnormal{where} \: \textnormal{head}_i = \textnormal{Attention}(QW_i^Q, KW^K_i, VW^V_i)
      \end{split}
    \end{equation*}
  \item \textbf{Position-wise Fead-Forward Networks:} 
    The output of the multi-head attention block is fed to a fully connected neural-network.
    This takes as input the representation at each position (making it ``position-wise'') and applies two linear transformations separated by a ReLU activation:
    \begin{equation*}
      \textnormal{FFN}(x) = \textnormal{max}(0, xW_1 + b_1)W_2 + b_2
    \end{equation*}
\end{itemize}
Multiple blocks consisting of the multi-head attention and feed-forward layers are stacked on top of one another to produce a deeper transformer model.
\subsubsection{Model Variants}
The encoder-decoder model introduced in \cite{vaswani2017attention} is not the only variant of the transformer architecture that exists.
Modifications to the transformer architecture have been made in order to better support different tasks:  
\begin{itemize}
  \item \textbf{Encoder-decoder:}
    With encoder-decoder models, an input sequence is first embedded, then added to a positional encoding before being passed as input to a stack of attention and feed-forward network layers (the ``encoder'') to produce a representation of the input sequence. 
    As the output sequence is generated, it is fed as input to a similarly structured ``decoder'' block. 
    This takes an embedded output sequence, with positional encoding, as input.
    The output of the encoder block is fed to attention blocks in the decoder block in order to model the relationship between the input and output sequence.
    These models \cite{raffel2020exploring} tend to be used for tasks like sequence to sequence transformation, for example language translation.
  \item \textbf{Encoder-only:}
    Encoder-only models use only the encoder block of an encoder-decoder model to produce a vector representation of an input sequence.
    These models do not produce text directly, but are instead meant to pass their output to a downstream component for further inference, potentially for applications to sentiment analysis or named-entity recognition.
    Examples of these include the BERT \cite{kenton2019bert} family of models.
  \item \textbf{Decoder-only:}
    Decoder-only models like those in the GPT \cite{radford2018improving} series make up the bulk of models used for auto-regressive text generation and completion. 
    Instead of sending an input prompt to an encoder block, the prompt is instead passed in in conjuction to the output sequence and the model is trained to predict the next token in the sequence \cite{dai2015semi}.
\end{itemize}

\subsection{LLM Inference}\label{subsection:llminference}
We begin with an overview of the steps taken to perform inference for a single request, before detailing other important aspects of LLM inference like managing the KV cache, distributing computation across multiple compute nodes and request batching.
\subsubsection{Inference Pipeline}\label{subsubsection:inferencepipeline}
Performing inference based on a user prompt involves a prefill phase to initialise the state of the transformer model followed by an autoregressive phase to generate an output sequence.
These steps are sandwiched by tokenisation and detokenisation stages respectively in order to convert between a string and vector representation of the input and output sequence.
\begin{itemize}
  \item \textbf{Tokenisation:}
    In order to convert from a string representation of an input sequence, to a vector representation that a model can understand, tokenisation is applied.
    This splits the input string into a sequence of tokens present in a vocabulary learnt from the training corpus \cite{radford2019language} \cite{sennrich2015neural}, producing a vector of indices for each token in the input string.
    These indices are used to index into the learnt embeddings matrix and extract an embedding vector for each input token.
  \item \textbf{Prefill phase:}
    The prefill phase refers to the stage in the inference pipeline at which the hidden states (keys and values) of the network from the input sequence are produced.
    These values are later used to generate output tokens.
    Additionally, KV cache entries (see \ref{subsubsection:kvcache}) for these tokens are produced and are later used to speed up the autoregressive phase of the inference pipeline.
    This stage represents an operation over a large matrix and, as such, can saturate the GPU, making this stage largely arithmetic bound.
  \item \textbf{Autoregressive phase:}
    In the autoregressive phase, output tokens are generated incrementally and appended back to the output prompt. 
    The final layer of the transformer produces a probability distribution over all output tokens (typically using a softmax activation), which is sampled from to produce an output token.
    Since we are only generating one token at a time, this phase is typically bound by the rate at which vectors can be passed to GPU memory.
    Further, the sequential dependencies between outputs limits the parallelisability of the autoregressive phase.
    The sequential nature of the autoregressive phase, and its corresponding performance characteristics, has significantly shaped the design of existing LLM runtimes (see \ref{section:relatedwork}).
  \item \textbf{Detokenisation:}
    At the end of the autoregressive phase, a vector containing indices into the model vocabulary is produced - this represents the output sequence of the model.
    In order to convert this vector to a string representation, the vector is sent to the CPU and each element is used to index into the model vocabulary, producing a string for each token.
    This list of token strings is then joined to form the overall output string.
\end{itemize}

\subsubsection{KV Cache}\label{subsubsection:kvcache}
The KV cache represents an important area with which inference efficiency can be improved.
During the autoregressive phase of inference, self-attention refers to the key and value vectors $KW_i^K$ and $VW_i^V$ as well as the query vector of the latest output token.
These key and value vectors have already been calculated at prior iterations and so caching them can reduce the computational overhead at each autoregressive step, as we can use these vectors without recalculation.
The computed key and value vectors for the newest token are then added to the KV cache, to be reused during the next autoregressive iteration.
This cache is initially populated during the prefill phase but then grows with the output sequence length. 
The effective management of the KV cache can be used to help overcome the limited parallelisability of the autoregressive phase of inference \cite{shi2024keep} \cite{pope2023efficiently}.

\subsubsection{Hardware}
The recent success of LLMs is in large part enabled by the use of Graphics Processing Units (GPUs) \cite{krizhevsky2012imagenet}.
GPUs are optimised for performing a large number of specialised operations in parallel, in contrast with Central Processing Units (CPUs), which are better suited to performing a more general set of operations sequentially.
Modern deep neural networks can be distilled down into a sequence of vector and matrix operations.
These tasks are highly parallelisable and specialised implementations like of linear algebra operations like cuBLAS \cite{cublas} exist to accelerate them on GPU hardware.
Other software layers like CUDA \cite{nickolls2008scalable} also exist and can be used to implement efficient versions of common deep learning functions.  
As a result of this, GPUs are well suited for the kind of workloads required by LLMs. 

At a high level, GPUs consist of a series of Streaming Multiprocessors (SMs), each containing multiple Stream Processors (SPs, also referred to as `cores') and some shared memory\cite{choquette2021nvidia}. 
Each core is designed to support a small set of relatively simple operations, like floating-point and integer arithmetic, as well as logical operations.
High-bandwidth memory (HBM) is used to transfer data between SMs, while the shared memory on each SM can be used to share common data between threads.
Tensors like weights and the KV cache are typically stored in HBM and then moved into the SMs at the time of computation \cite{pope2023efficiently}.

% explore other hardware platforms - asics, fpgas, tpus
In addition to GPUs, other hardware platforms like ASICs, FPGAs and TPUs exist and can be used to further accelerate the operations of LLMs \cite{li2024large}.
These platforms tend to be more heterogenous in architecture and, as such, most existing LLM runtimes (see \ref{section:relatedwork}) tend to be optimised for a GPU-like architecture.

\subsubsection{Distributed Computation}
One approach to scaling LLMs to is to distribute computation amongst multiple GPUs. 
Model data can be shared across multiple nodes in a network to increase inference throughput, while model layers and even individual tensors can be sharded to enable the deployment of larger models.
These techniques need not be used in isolation and are frequently used in combination in existing training and inference pipelines \cite{kwon2023efficient} \cite{shoeybi2019megatron} \cite{yu2022orca}.
\begin{itemize}
  \item \textbf{Data parallelism:} Data parallelism involves replicating the weights for an entire model across multiple nodes.
    Each node is now capable of handling inference requests independently, increasing the number of requests that can be serviced in parallel.  
    This approach reduces request latency and is simple to implement and scale, but has high memory requirements, as each node must be capable of storing the weights for the entire model as well as any intermediate tensors produced during inference.
    This is potentially prohibitive for larger models featuring high numbers of weights \cite{rae2021scaling} \cite{brown2020language} \cite{chowdhery2023palm} .
  \item \textbf{Pipeline parallelism:} Pipeline parallelism is a way to distribute computation that enables training and inference for models with a large number of weights.
    This approach borrows from the classic computer architecture technique of pipelining \cite{hennessy2011computer}.
    Nodes are assigned the weight tensors for individual layers of the network and are responsible for executing that layer alone.
    Data is then passed between nodes in sequence \cite{huang2019gpipe}.
    This method incurs a small computational overhead in moving data between nodes in the pipeline, however this is relatively minor compared to the scheduling delay incurred in other methods like tensor parallelism.
    The main drawback of this method is the potential for bubbles to form in the pipeline.
    This occurs due to nodes taking different times to compute their layer.
    Nodes that complete early might have to wait idle while nodes upstream of them in the pipeline execute, reducing node utilisation.
  \item \textbf{Tensor parallelism:} Tensor parallelism extends pipeline parallelism to allow the deployment of models whose layers or intermediate tensors may be too large to fit on just one node.
    Layer specific tensors, like weights or intermediate activations can be sharded across multiple nodes. 
    Nodes then work together to complete a subset of the overall tensor computation before combining their results later.
    While this approach enables even larger models to be executed, it further increases the computational overhead of sharing data across nodes - we now need to synchronise and move data on an intra-layer basis, instead of on an inter-layer basis as in pipeline parallelism \cite{shoeybi2019megatron}.
\end{itemize}

\subsubsection{Request Batching}
Another method used to address the limited parallelism created by the autoregressive phase of inference is to batch requests. 
With request batching, we can stack tensors from multiple user requests into the same batch and issue this request to the GPU.
This enables user requests to be served in parallel and increases GPU utilisation at the same time as maximising use of available memory bandwith.

\begin{itemize}
  \item \textbf{Naive batching:} The naive approach to batching consists of grouping multiple input prompts into a batch and running inference on it - inference is scheduled at the granularity of requests \cite{yu2022orca}.
    In order for the input and output batches to be valid matrix shapes, they must be padded with special padding tokens to a fixed size.
    While simple to implement, this approach can lead to increased latency as well as underutilisation of the GPU. 
    This is because prompts within a batch do not necessarily generate output sequences of the same length. 
    If one output sequence has terminated while there are other sequences for which tokens are still being generated it will be unable to be returned to the user until all other sequences terminate.
    As such the request latency is bounded by the time taken to generate the longest sequence in the batch \cite{yu2022orca}.
    This also limits the system from serving new requests as they arrive: the new request will need to wait for completion of the entire batch of requests, even if the batch contains requests for which an output has already been generated.
    As a result, naive batching is limited in its use, only achieving maximal GPU utilisation under very specific usage patterns.

  \item \textbf{Continuous batching:} Continuous batching is proposed as an improvement to naive batching and schedules inference at the granularity of autoregressive iterations \cite{yu2022orca}.
    At the end of each autoregressive iteration, a set of new tokens is produced for every request in the batch.    
    A scheduler then monitors the batch to determine the completion status of every request in the batch, dynamically scheduling waiting requests when it detects that a complete output sequence has been generated.
    This achieves greater utilisation of the GPU since it is no longer executing operations for sequences that have already terminated.
    It also reduces latency for user requests, as output sequences can be returned as soon as they are terminated.
    However, this batching technique comes at the cost of increased scheduling overhead as well as limited batching of certain matrix operations.
    This is because some operations, like computing attention, require their input sequences to be aligned to the same length and position in order to be batched.
    Because requests are dynamically dispatched, this property no longer holds and thus operations like computing attention blocks are unabled to be batched, in a technique known as ``selective batching'' \cite{yu2022orca}.
\end{itemize}


\begin{comment}
\subsubsection{Kernel Specialisation}
% fused kernels
% flashattention

\subsubsection{Sampling Strategy}\label{subsubsection:samplingstrategy}
% https://medium.com/@shashankag14/understanding-sampling-techniques-in-large-language-models-llms-dfc28b93f518

\subsubsection{Quantization}
% Keep the Cost Down: A Review on Methods to Optimize LLM’s KV Cache Consumption
% speak about how quantization can increase the effective size of the KV cache
\end{comment}

\section{Related Work}\label{section:relatedwork}
% speak about different runtimes and their contributions
% compare performance and justify vllm as open source alternative from which to base work off of
Various runtimes for performing LLM inference already exist.
These implement some of the techniques described earlier in order to provide efficient inference for a variety of use cases.
In order to understand where it might be possible to offer a novel contribution with regard to performance, we first need to understand the features offered by existing solutions.
Here we present a selection of popular runtimes and detail their main contributions and points of difference.
\subsection{vLLM}
vLLM is a popular LLM runtime designed for high throughput serving of inference requests \cite{kwon2023efficient}.
It addresses key challenges in memory management that arise during LLM serving, particularly those related to KV cache memory.
Existing systems often suffer from memory inefficiencies due to fragmentation and over-allocation, limiting their ability to process large batch sizes effectively.
vLLM solves this problem through a novel memory management mechanism, achieving near-zero KV cache waste \cite{kwon2023efficient}.
\subsubsection{Key Contributions}

The main contribution of vLLM is its PagedAttention mechanism. 
On receiving a new request, many inference runtimes pre-allocate a contiguous chunk of memory for the KV cache equal in size to the request's maximum output.
This can cause internal fragmentation, if the actual output is less than the size of memory allocated, as well as external fragmentation, when smaller requests are unable to be scheduled due to overallocation.
Moreover, for requests that generate multiple outputs, as is the case with parallel and beam search, the KV cache is stored in a different location for each output and so there is no possibility of memory reuse.

To address this, PagedAttention divides the request's KV cache into smaller units called blocks. 
These contain a fixed number of key and value vectors associated with a set of tokens. 
Blocks for a given request's KV cache need not be located in physically contiguous locations and are allocated dynamically as the KV cache for a request grows. 
This eliminates external fragmentation as each block is the same size.
The blocks themselves are chosen to be small in size in order to limit internal fragmentation \cite{kwon2023efficient}.
This is analagous to the paging technique common to modern operating systems \cite{kilburn1962one}, with blocks corresponding to pages, tokens to bytes and requests to processes \cite{kwon2023efficient}.
By optimising memory allocation, vLLM allows larger batch sizes, significantly improving throughput.

The rest of the vLLM system is built to support this use of PagedAttention and integrates several other common optimisation techniques, including continuous batching, as well as pipeline and tensor parallelism.
It supports a variety of popular LLMs, including those in the GPT, OPT and LLaMA series \cite{radford2018improving} \cite{zhang2022opt} \cite{touvron2023llama}.
The PagedAttention optimisation also integrates well with popular decoding techniques like parallel and beam search, facilitating memory sharing across request outputs \cite{kwon2023efficient}.

\subsubsection{Architecture}
vLLM exposes a Python library and an OpenAI API \cite{openaiapi} compatible web server for inference.
The frontend is written using the FastAPI library \cite{fastapi}.
Python is used to implement the bulk of the project, with C++ and CUDA code being used to implement the custom CUDA kernels required to support the PagedAttention mechanism.
Importantly, control-specific components like the batch scheduler are implemented in Python, representing a potential source of overhead.

vLLM uses a heirarchical class structure to distribute requests amongst GPU workers.
A single executor is maintained through the lifespan of the runtime.
This is responsible for creating and running worker processes.
These are processes dedicated to running model inference on specific GPUs and are tasked with preparing tensors and orchestrating the execution of the model on that node. 
Importantly, this means that a separate Python process needs to be created for every single worker node, incurring a performance penalty.

On individual GPU workers, a block engine is used to allocate a contiguous chunk of GPU RAM and divide it into physical KV cache blocks.
A block manager is created to maintain the mapping between logical and physical KV blocks and this is also implemented in Python. 
NCCL \cite{nccl} is used for communication across distributed GPU workers.

\subsection{TensorRT-LLM}
NVIDIA's TensorRT-LLM extends the TensorRT deep learning compiler to provide an inference runtime for LLMs \cite{nvidiainferencewhitepaper}.
The TensorRT compiler is a high-performance deep learning inference optimizer designed to transform trained neural network models into highly efficient computational graphs suitable for deployment on NVIDIA hardware. 
It achieves this by applying a range of optimizations as well as making use of NVIDIA-specific hardware features \cite{nvidiainferencewhitepaper}. 
This integration with TensorRT, and its associated performance on NVIDIA products is the principal advantage of this runtime.

TensorRT-LLM provides a Python library similar to that of vLLM through which inference is performed.
Similarly to vLLM, TensorRT-LLM also implements pipeline and tensor parallelism as well as continuous batching \cite{nvidiainferencewhitepaper}.

% \subsection{Orca}
\subsection{llama.cpp} \label{section:llamacpp}
% brief explanation of llama
% feature explanation


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Project Plan} % 1-2 pages
% Break down runtime implementation (this probably requires a bit of knowledge about how llama.cpp works)
% I think here is where I break down vllm a bit more thoroughly and link it to llama.cpp?
% 

\begin{itemize}
  \item \textbf{Identifying inefficiencies:}
  \item \textbf{Project report:}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Evaluation} % 1-2 pages
\begin{comment}
1. Run some benchmark on llama.cpp
2. Run same benchmark on llama.cpp + my system
3. ...
4. Profit???

Key requirements for project

Areas on which I could track performance relative to existing system
- Efficiency (\% of overall resources used)
- Throughput (tokens/sec)

\end{comment}
% llama-bench (llama cpp thing)

Use ShareGPT [cite] and Alpaca [cite] datasets

\section{Functional Requirements}
% Implement OpenAI API
\section{Performance Metrics}
\begin{itemize}
  \item \textbf{Throughput (tokens/s):}
  \item \textbf{Time to First Token (s):}
  \item \textbf{Time Per Output Token (ms):}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Ethical Issues} % 1-2 pages
The principal two ethical concerns of a project in this field relate to the potential for misuse as well as provenance issues surrounding the dataset on which the model was trained.

The potential for misuse of LLMs is vast, with many instances of LLM abuse already being documented.
LLM abuse typically involves the use of the model to produce harmful or misleading content.
There already exist proof-of-concepts for LLMs \cite{hazell2023spear} being used to generate phishing messages, with the intent to produce emails that sound more plausible and are more likely to be engaged with by a target.
In addition to this, LLMs can be used to produce vast quantities misinformation or biased content that are then published to social media platforms \cite{williams2024large}. 
End users may be unable to distinguish between content created by a genuine user and content generated by an LLM and thus end up misinformed.

The large size of the datasets required to train these models create potential ethical and data protection issues.
Concerns exist regarding the ability for generative models to amplify existing biases in their training data, with some of these concerns borne out in cases like Microsoft's Tay chatbot \cite{tayMicrosoft}.
AI fairness is still an open area of research \cite{xivuri2021systematic} and it is unlikely that existing LLM models will be completely free of bias at inference time.
At the same time, the provenance of this training data is also an important ethical consideration.
Private or sensitive data has the potential to be incorporated into training sets and there exist cases \cite{nasr2023scalable} where this training data has then been generated verbatim at inference time, exposing this sensitive data to an end user.

If successful, our project broadens access to LLMs by making better use of available hardware to perform inference.
This increases the viability of local inference and opens up these models to a greater proportion of hardware configurations and thus a greater number of users.
While this represents a boon for the accessibility of this technology, with users no longer limited to a handful of offerings by large companies, it also increases the number of potentially malicious actors who are able to use LLMs.
Small and local deployments likely have less of the oversight that large LLM providers experience, and thus are more able to misuse this technology.
These two elements must be carefully managed in order to produce a project that adheres to reasonable ethical standards.

As this project represents a proof-of-concept, rather than a full-featured inference engine, any advances made are unlikely to immediately be adopted and thus any ethical concerns are likely to be uncovered at a pace with which they can be identified early on and mitigated quickly.


%% bibliography
\bibliographystyle{vancouver}
\bibliography{references}


\end{document}
