\documentclass[12pt,twoside]{report}

% some definitions for the title page
\newcommand{\reporttitle}{A New Scalable Runtime for LLM Inference}
\newcommand{\reportauthor}{Hamish McCreanor}
\newcommand{\supervisor}{Peter Pietzuch}
\newcommand{\reporttype}{MEng Individual Project}
\newcommand{\degreetype}{MEng Computing} 

% load some definitions and default packages
\input{includes}

% load some macros
\input{notation}

% load title page
\begin{document}
\input{titlepage}


% page numbering etc.
\pagenumbering{roman}
\clearpage{\pagestyle{empty}\cleardoublepage}
\setcounter{page}{1}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Your abstract.
\end{abstract}

\cleardoublepage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
\section*{Acknowledgments}
Comment this out if not needed.

%\clearpage{\pagestyle{empty}\cleardoublepage}
\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--- table of contents
\fancyhead[RE,LO]{\sffamily {Table of Contents}}
\tableofcontents 


%\clearpage{\pagestyle{empty}\cleardoublepage}
\pagenumbering{arabic}
\setcounter{page}{1}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction} % 1-3 pages
% What is the problem?
% Why is it interesting?
% How do I propose to solve it?
\begin{comment}
  
\end{comment}

As large language models (LLMs) are adopted for an ever-growing range of applications, scaling time spent in training has emerged as the dominant paradigm for improving model performance.
However, with training costs growing prohibitively expensive, there is increasing interest in other techniques for bettering model output. 
Test-time compute scaling offers one such way to improve performance, with existing models achieving better results by ``thinking'' for longer.
This is done by encouraging LLMs to output longer chains of reasoning, shifting scaling costs from training time to test time and enabling existing models to improve without incurring any extra training expense.

The most popular test-time compute scaling approaches leverage a second model to guide a tree-search towards the best answer.
A generator LLM proposes steps to an answer to a user query and a verifier LLM scores these steps, with this process repeating until a set criteria is met. 
This is a departure from an assumption made by existing LLM inference systems, which is that only one model will run at a time.
As such, they are likely to have made architectural decisions that limit the throughput and latency of a test-time scaling approach involving the interplay between multiple LLMs.

This represents an opportunity to build a new system that more effectively supports the test-time scaling use case.
Key to these efforts is optimizing the memory of such a system, particularly the management of the key-value (KV) cache, which stores intermediate results reused during autoregressive token generation. 
Efficient use of the KV cache can significantly improve memory utilization and computational speed.

This project analyses a beam search implementation built upon vLLM, a popular and widely used LLM inference library, to try and identify deficiencies that limit the effective use of GPU memory. 
Several experiments are conducted to better understand how the implementation's performance changes as a function of various parameters and optimisations to improve GPU memory utilisation are proposed and validated. 

%%%% Add bit here explaining what optimisations I tried

%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background} % 10-20 pages
We provide an overview of transformer architecture in \ref{section:llmarchitecture}, as well as key components of the inference pipeline in \ref{section:llminference}, before detailing the idea of test-time compute scaling and describing relevant techniques in \ref{section:testtimecompute}.
\section{LLM Architecture}\label{section:llmarchitecture}
\subsection{Transformer Architecture}
While the term ``Large Language Model'' can be used to describe any model trained on large volumes of textual data, it is frequently used to refer to models that use a variant of the transformer architecture described in \cite{vaswani2017attention}.
This architecture has superseded recurrent neural networks for language-based tasks owing to its ability to capture long range dependencies between tokens.
It does this via a unique attention mechanism.
This, and the other components present in a transformer, are described below.
\begin{itemize}
  \item \textbf{Embedding Layers:} 
    Attention blocks are fed a matrix where each row represents the semantic meaning of each token. This is done by using an embedding learnt during the training process and applying this to the input tokens.
  \item \textbf{Positional Encoding:} 
    Unlike recurrent neural networks, transformer models contain no a priori knowledge of the order of the input sequence.
    To remedy this, positional encodings are added to the input embeddings before they are fed to the attention block.
    There are several ways of doing this, however the approach adopted in the \cite{vaswani2017attention} is to use apply sine and cosine functions to the position and dimension of the input vector as follows:
    \begin{equation*}
      \begin{split}
        \textnormal{PE}_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}) \\ 
        \textnormal{PE}_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})
      \end{split}
    \end{equation*}
  \item \textbf{Multi-Head Attention:}
    Transformers use an attention block to describe the relationship between different tokens in a sequence. 
    This takes as input a key, query and value matrix $K$, $Q$, $V$ respectively. 
    The key and query matrices are multiplied together to produce a representation of how each token in the sequence relates to each other token.
    After normalising and applying a softmax function, the matrix is multiplied by $V$, which represents the semantic meaning of each input token.
    \begin{equation*}
      \textnormal{Attention}(Q,K,V) = \textnormal{softmax}(\frac{QK^T}{\sqrt{d_k}})V
    \end{equation*}
    In practise, multiple attention heads are used and their outputs concatenated.  
    Instead of attention blocks just operating on the $K$, $Q$, and $V$ directly, they are multiplied by learned matrices $W^K$, $W^Q$, and $W^V$ to project the input vectors into different spaces.
    The fact that there are multiple of these matrices $W_i^Q$, $W_i^K$ and $W_i^V$ allows the model to attend to information learnt from different projections concurrently.
    \begin{equation*}
      \begin{split}
        \textnormal{MultiHead}(Q,K,V) = \textnormal{Concat}(\textnormal{head}_1, ..., \textnormal{head}_h)W^O \\
        \textnormal{where} \: \textnormal{head}_i = \textnormal{Attention}(QW_i^Q, KW^K_i, VW^V_i)
      \end{split}
    \end{equation*}
  \item \textbf{Position-wise Fead-Forward Networks:} 
    The output of the multi-head attention block is fed to a fully connected neural-network.
    This takes as input the representation at each position (making it ``position-wise'') and applies two linear transformations separated by a ReLU activation:
    \begin{equation*}
      \textnormal{FFN}(x) = \textnormal{max}(0, xW_1 + b_1)W_2 + b_2
    \end{equation*}
\end{itemize}
Multiple blocks consisting of the multi-head attention and feed-forward layers are stacked on top of one another to produce a deeper transformer model.
\subsubsection{Model Variants}
The encoder-decoder model introduced in \cite{vaswani2017attention} is not the only variant of the transformer architecture that exists.
Modifications to the transformer architecture have been made in order to better support different tasks:  
\begin{itemize}
  \item \textbf{Encoder-decoder:}
    With encoder-decoder models, an input sequence is first embedded, then added to a positional encoding before being passed as input to a stack of attention and feed-forward network layers (the ``encoder'') to produce a representation of the input sequence. 
    As the output sequence is generated, it is fed as input to a similarly structured ``decoder'' block. 
    This takes an embedded output sequence, with positional encoding, as input.
    The output of the encoder block is fed to attention blocks in the decoder block in order to model the relationship between the input and output sequence.
    These models \cite{raffel2020exploring} tend to be used for tasks like sequence to sequence transformation, for example language translation.
  \item \textbf{Encoder-only:}
    Encoder-only models use only the encoder block of an encoder-decoder model to produce a vector representation of an input sequence.
    These models do not produce text directly, but are instead meant to pass their output to a downstream component for further inference, potentially for applications to sentiment analysis or named-entity recognition.
    Examples of these include the BERT \cite{kenton2019bert} family of models.
  \item \textbf{Decoder-only:}
    Decoder-only models like those in the GPT \cite{radford2018improving} series make up the bulk of models used for auto-regressive text generation and completion. 
    Instead of sending an input prompt to an encoder block, the prompt is instead passed in in conjuction to the output sequence and the model is trained to predict the next token in the sequence \cite{dai2015semi}.
\end{itemize}

\section{LLM Inference}\label{section:llminference}
We begin with an overview of the steps taken to perform inference for a single request, before detailing other important aspects of LLM inference like managing the KV cache, distributing computation across multiple compute nodes and request batching.
\subsection{Inference Pipeline}\label{subsection:inferencepipeline}
Performing inference based on a user prompt involves a prefill phase to initialise the state of the transformer model followed by an autoregressive phase to generate an output sequence.
These steps are sandwiched by tokenisation and detokenisation stages respectively in order to convert between a string and vector representation of the input and output sequence.
\begin{itemize}
  \item \textbf{Tokenisation:}
    In order to convert from a string representation of an input sequence, to a vector representation that a model can understand, tokenisation is applied.
    This splits the input string into a sequence of tokens present in a vocabulary learnt from the training corpus \cite{radford2019language} \cite{sennrich2015neural}, producing a vector of indices for each token in the input string.
    These indices are used to index into the learnt embeddings matrix and extract an embedding vector for each input token.
  \item \textbf{Prefill phase:}
    The prefill phase refers to the stage in the inference pipeline at which the hidden states (keys and values) of the network from the input sequence are produced.
    These values are later used to generate output tokens.
    Additionally, KV cache entries (see \ref{subsection:kvcache}) for these tokens are produced and are later used to speed up the autoregressive phase of the inference pipeline.
    This stage represents an operation over a large matrix and, as such, can saturate the GPU, making this stage largely arithmetic bound.
  \item \textbf{Autoregressive phase:}
    In the autoregressive phase (also referred to as the decode phase), output tokens are generated incrementally and appended back to the output prompt. 
    The final layer of the transformer produces a probability distribution over all output tokens (typically using a softmax activation), which is sampled from to produce an output token.
    A temperature parameter can be introduced to modify this probability distribution and control the level of randomness present in the sampling stage.
    Since we are only generating one token at a time, this phase is typically bound by the rate at which vectors can be passed to GPU memory.
    Further, the sequential dependencies between outputs limits the parallelisability of the autoregressive phase.
    The sequential nature of the autoregressive phase, and its corresponding performance characteristics, has significantly shaped the design of existing LLM runtimes (see \ref{section:relatedwork}).
  \item \textbf{Detokenisation:}
    At the end of the autoregressive phase, a vector containing indices into the model vocabulary is produced - this represents the output sequence of the model.
    In order to convert this vector to a string representation, the vector is sent to the CPU and each element is used to index into the model vocabulary, producing a string for each token.
    This list of token strings is then joined to form the overall output string.
\end{itemize}

\subsection{KV Cache}\label{subsection:kvcache}
The KV cache represents an important area with which inference efficiency can be improved.
During the autoregressive phase of inference, self-attention refers to the key and value vectors $KW_i^K$ and $VW_i^V$ as well as the query vector of the latest output token.
These key and value vectors have already been calculated at prior iterations and so caching them can reduce the computational overhead at each autoregressive step, as we can use these vectors without recalculation.
The computed key and value vectors for the newest token are then added to the KV cache, to be reused during the next autoregressive iteration.
This cache is initially populated during the prefill phase but then grows with the output sequence length. 
The effective management of the KV cache can be used to help overcome the limited parallelisability of the autoregressive phase of inference \cite{shi2024keep} \cite{pope2023efficiently}.

\subsection{Hardware}
The recent success of LLMs is in large part enabled by the use of Graphics Processing Units (GPUs) \cite{krizhevsky2012imagenet}.
GPUs are optimised for performing a large number of specialised operations in parallel, in contrast with Central Processing Units (CPUs), which are better suited to performing a more general set of operations sequentially.
Modern deep neural networks can be distilled down into a sequence of vector and matrix operations.
These tasks are highly parallelisable and specialised implementations like of linear algebra operations like cuBLAS \cite{cublas} exist to accelerate them on GPU hardware.
Other software layers like CUDA \cite{nickolls2008scalable} also exist and can be used to implement efficient versions of common deep learning functions.  
As a result of this, GPUs are well suited for the kind of workloads required by LLMs. 

At a high level, GPUs consist of a series of Streaming Multiprocessors (SMs), each containing multiple Stream Processors (SPs, also referred to as `cores') and some shared memory\cite{choquette2021nvidia}. 
Each core is designed to support a small set of relatively simple operations, like floating-point and integer arithmetic, as well as logical operations.
High-bandwidth memory (HBM) is used to transfer data between SMs, while the shared memory on each SM can be used to share common data between threads.
Tensors like weights and the KV cache are typically stored in HBM and then moved into the SMs at the time of computation \cite{pope2023efficiently}.

% explore other hardware platforms - asics, fpgas, tpus
In addition to GPUs, other hardware platforms like ASICs, FPGAs and TPUs exist and can be used to further accelerate the operations of LLMs \cite{li2024large}.
These platforms tend to be more heterogenous in architecture and, as such, most existing LLM runtimes (see \ref{section:relatedwork}) tend to be optimised for a GPU-like architecture.

\subsection{Distributed Computation}
One approach to scaling LLMs to is to distribute computation amongst multiple GPUs. 
Model data can be shared across multiple nodes in a network to increase inference throughput, while model layers and even individual tensors can be sharded to enable the deployment of larger models.
These techniques need not be used in isolation and are frequently used in combination in existing training and inference pipelines \cite{kwon2023efficient} \cite{shoeybi2019megatron} \cite{yu2022orca}.
\begin{itemize}
  \item \textbf{Data parallelism:} Data parallelism involves replicating the weights for an entire model across multiple nodes.
    Each node is now capable of handling inference requests independently, increasing the number of requests that can be serviced in parallel.  
    This approach reduces request latency and is simple to implement and scale, but has high memory requirements, as each node must be capable of storing the weights for the entire model as well as any intermediate tensors produced during inference.
    This is potentially prohibitive for larger models featuring high numbers of weights \cite{rae2021scaling} \cite{brown2020language} \cite{chowdhery2023palm} .
  \item \textbf{Pipeline parallelism:} Pipeline parallelism is a way to distribute computation that enables training and inference for models with a large number of weights.
    This approach borrows from the classic computer architecture technique of pipelining \cite{hennessy2011computer}.
    Nodes are assigned the weight tensors for individual layers of the network and are responsible for executing that layer alone.
    Data is then passed between nodes in sequence \cite{huang2019gpipe}.
    This method incurs a small computational overhead in moving data between nodes in the pipeline, however this is relatively minor compared to the scheduling delay incurred in other methods like tensor parallelism.
    The main drawback of this method is the potential for bubbles to form in the pipeline.
    This occurs due to nodes taking different times to compute their layer.
    Nodes that complete early might have to wait idle while nodes upstream of them in the pipeline execute, reducing node utilisation.
  \item \textbf{Tensor parallelism:} Tensor parallelism extends pipeline parallelism to allow the deployment of models whose layers or intermediate tensors may be too large to fit on just one node.
    Layer specific tensors, like weights or intermediate activations can be sharded across multiple nodes. 
    Nodes then work together to complete a subset of the overall tensor computation before combining their results later.
    While this approach enables even larger models to be executed, it further increases the computational overhead of sharing data across nodes - we now need to synchronise and move data on an intra-layer basis, instead of on an inter-layer basis as in pipeline parallelism \cite{shoeybi2019megatron}.
\end{itemize}

\subsection{Request Batching}
Another method used to address the limited parallelism created by the autoregressive phase of inference is to batch requests. 
With request batching, we can stack tensors from multiple user requests into the same batch and issue this request to the GPU.
This enables user requests to be served in parallel and increases GPU utilisation at the same time as maximising use of available memory bandwith.

\begin{itemize}
  \item \textbf{Naive batching:} The naive approach to batching consists of grouping multiple input prompts into a batch and running inference on it - inference is scheduled at the granularity of requests \cite{yu2022orca}.
    In order for the input and output batches to be valid matrix shapes, they must be padded with special padding tokens to a fixed size.
    While simple to implement, this approach can lead to increased latency as well as underutilisation of the GPU. 
    This is because prompts within a batch do not necessarily generate output sequences of the same length. 
    If one output sequence has terminated while there are other sequences for which tokens are still being generated it will be unable to be returned to the user until all other sequences terminate.
    As such the request latency is bounded by the time taken to generate the longest sequence in the batch \cite{yu2022orca}.
    This also limits the system from serving new requests as they arrive: the new request will need to wait for completion of the entire batch of requests, even if the batch contains requests for which an output has already been generated.
    As a result, naive batching is limited in its use, only achieving maximal GPU utilisation under very specific usage patterns.

  \item \textbf{Continuous batching:} Continuous batching is proposed as an improvement to naive batching and schedules inference at the granularity of autoregressive iterations \cite{yu2022orca}.
    At the end of each autoregressive iteration, a set of new tokens is produced for every request in the batch.    
    A scheduler then monitors the batch to determine the completion status of every request in the batch, dynamically scheduling waiting requests when it detects that a complete output sequence has been generated.
    This achieves greater utilisation of the GPU since it is no longer executing operations for sequences that have already terminated.
    It also reduces latency for user requests, as output sequences can be returned as soon as they are terminated.
    However, this batching technique comes at the cost of increased scheduling overhead as well as limited batching of certain matrix operations.
    This is because some operations, like computing attention, require their input sequences to be aligned to the same length and position in order to be batched.
    Because requests are dynamically dispatched, this property no longer holds and thus operations like computing attention blocks are unabled to be batched, in a technique known as ``selective batching'' \cite{yu2022orca}.
\end{itemize}

\section{Test Time Compute Scaling}\label{section:testtimecompute}
Historically, approaches to improving LLM performance have centered on training. 
With larger datasets, larger models and more training epochs, performance on the cross-entropy loss function used for training these models can be made to improve considerably \cite{kaplan2020scaling}.  
However, scaling in this way eventually becomes prohibitively expensive due to the high cost of owning and operating the hardware required for training.

This motivates the idea of test-time compute scaling.
Test-time compute scaling trades off inference latency for higher quality responses to user prompts. 
By allowing existing LLMs to take longer to respond to user prompts, the quality of responses can be improved considerably \cite{snell2024scaling}, obviating the need to retrain a bigger model on a more expansive dataset.
Test-time compute scaling describes a broad approach to trading off inference latency for higher quality model outputs - specific methodologies for how to implement this are described below.

\subsection{Chain of Thought Prompting}
Chain of thought prompting uses a series of reasoning steps to improve LLM answer quality.  
By prompting a model with examples of answers containing structured reasoning steps and encouraging it to follow a similar structure in its own responses, model performance has been empirically shown to improve considerably \cite{wei2022chain}.

\subsection{Self Consistency Sampling}
Self-consistency sampling is a simple technique to improve model outputs that replaces the decoding strategy present in chain of thought prompting.
Instead of greedily sampling each token, self-consistency voting generates a diverse set of reasoning paths by sampling multiple tokens in parallel.
These tokens produce a tree structure, with different branches corresponding to different token choices.
When all reasoning paths conclude with an answer to the user's question, the answers from all paths are aggregated out by majority vote to produce a final response \cite{wang2022self}.

\subsection{Best-of-N Sampling}
Best-of-N sampling extends the idea of self-consistency to include some sort of external verifier.
This verifier can be any machine learning model but is typically some sort of transformer model.
In the basic best-of-n setup, this verifier scores all the n solutions sampled from the LLM and the one with the highest score is output as the final response.
With weighted best-of-n, all the final solutions are aggregated such that reasoning paths that generate the same solution are weighted higher when combined with the verifier score \cite{stiennon2020learning}.  

\subsection{Beam Search}
Beam search provides a way of systematically exploring the tree structure created when generating multiple chains of reasoning in parallel. 
Multiple chains of reasoning are generated iteratively, with scores being output for each step.
These scores are then used to guide the search towards higher quality answers - reasoning steps with higher scores are expanded through further sampling and reasoning steps with lower scores are pruned \cite{feng2023alphazero} \cite{yao2023tree}. 

We maintain a fixed number $N$ of active paths and expand each of these paths with $M$ samples. 
All of these $N \times M$ new reasoning steps are scored and the top $N$ steps are kept for the next iteration. 

Implementations of beam search frequently use a Process Reward Model (PRM) to score the intermediate reasoning steps. 
PRMs differ from the verifier models used in best-of-n search in that they are trained to output a score for every reasoning step, rather than just the final result.

%% Include a diagram of some sort

\chapter{Related Work}
% speak about different runtimes and their contributions
Various runtimes for performing LLM inference already exist.
These implement some of the techniques described earlier in order to provide efficient inference for a variety of use cases.
In order to understand where it might be possible to offer a novel contribution with regard to performance, we first need to understand the features offered by existing solutions.
Here we present a selection of popular runtimes and detail their main contributions and points of difference.
\section{Inference Runtimes}\label{section:inferenceruntimes}
\subsection{vLLM}
vLLM is a popular LLM runtime designed for high throughput serving of inference requests \cite{kwon2023efficient}.
It addresses key challenges in memory management that arise during LLM serving, particularly those related to KV cache memory.
Existing systems often suffer from memory inefficiencies due to fragmentation and over-allocation, limiting their ability to process large batch sizes effectively.
vLLM solves this problem through a novel memory management mechanism called PagedAttention, achieving near-zero KV cache waste \cite{kwon2023efficient}.
The rest of the vLLM system is built to support this use of PagedAttention and integrates several other common optimisation techniques, including continuous batching, as well as pipeline and tensor parallelism.
It supports a variety of popular LLMs, including those in the GPT, OPT and LLaMA series \cite{radford2018improving} \cite{zhang2022opt} \cite{touvron2023llama}.

\subsubsection{PagedAttention}
The main contribution of vLLM is its PagedAttention mechanism. 
On receiving a new request, many inference runtimes pre-allocate a contiguous chunk of memory for the KV cache equal in size to the request's maximum output.
This can cause internal fragmentation, if the actual output is less than the size of memory allocated, as well as external fragmentation, when smaller requests are unable to be scheduled due to overallocation.
Moreover, for requests that generate multiple outputs, as is the case with parallel and beam search, the KV cache is stored in a different location for each output and so there is no possibility of memory reuse.

To address this, PagedAttention divides the request's KV cache into smaller units called blocks. 
These contain a fixed number of key and value vectors associated with a set of tokens. 
Blocks for a given request's KV cache need not be located in physically contiguous locations and are allocated dynamically as the KV cache for a request grows. 
This eliminates external fragmentation as each block is the same size.
The blocks themselves are chosen to be small in size in order to limit internal fragmentation \cite{kwon2023efficient}.
This is analagous to the paging technique common to modern operating systems \cite{kilburn1962one}, with blocks corresponding to pages, tokens to bytes and requests to processes \cite{kwon2023efficient}.

By passing only the necessary KV cache blocks to the GPU kernel for each request, vLLM avoids overallocation and enables tighter packing of data from multiple requests into a single kernel invocation. 
This effectively increases the batch size and leads to significantly improved throughput.

\subsubsection{Automatic Prefix Caching}
The KV cache blocks used by PagedAttention provide a natural mechanism through which prefix caching can be implemented.
Prefix caching is a technique common to many inference runtimes to avoid recomputing KV cache values across shared input prompts.
The KV cache blocks of processed requests are cached and reused when a new request comes in with a shared prefix.

\textbf{Implementation}
vLLM introduces a KV cache manager to handle this automatic prefix caching.
It maintains a doubly linked list of free KV cache blocks, as well as a list of currently cached KV blocks in use.

When a new request arrives, vLLM computes hash values for the token sequences in the request - these hashes are derived not only from the current tokens but also from the sequence of prefix tokens that precedes them. 
If a matching hash is found, indicating that the KV cache blocks for those tokens have already been computed, they can be directly reused by the new request.

In the event of a prefix cache miss, vLLM allocates new KV cache blocks by popping from the list of free blocks and populating them with the computed KV values for the unmatched prefix.

If no free KV cache blocks are available, vLLM evicts blocks from the cache. 
Eviction is done based on a least-recently-used (LRU) policy, with a preference for evicting the blocks associated with the longest prefixes first, as these are less likely to be shared by future requests.
% diagram


\subsubsection{Architecture}
vLLM exposes a Python library and an OpenAI API \cite{openaiapi} compatible web server for inference.
The frontend is written using the FastAPI library \cite{fastapi}.
Python is used to implement the bulk of the project, with C++ and CUDA code being used to implement the custom CUDA kernels required to support the PagedAttention mechanism.
Importantly, control-specific components like the batch scheduler are implemented in Python, representing a potential source of overhead.

vLLM uses a heirarchical class structure to distribute requests amongst GPU workers.
A single executor is maintained through the lifespan of the runtime.
This is responsible for creating and running worker processes.
These are processes dedicated to running model inference on specific GPUs and are tasked with preparing tensors and orchestrating the execution of the model on that node. 
Importantly, this means that a separate Python process needs to be created for every single worker node, incurring a performance penalty.

On individual GPU workers, a block engine is used to allocate a contiguous chunk of GPU RAM and divide it into physical KV cache blocks.
A block manager is created to maintain the mapping between logical and physical KV blocks and this is also implemented in Python. 
NCCL \cite{nccl} is used for communication across distributed GPU workers.

\subsection{TensorRT-LLM}
NVIDIA's TensorRT-LLM extends the TensorRT deep learning compiler to provide an inference runtime for LLMs \cite{nvidiainferencewhitepaper}.
The TensorRT compiler is a high-performance deep learning inference optimizer designed to transform trained neural network models into highly efficient computational graphs suitable for deployment on NVIDIA hardware. 
It achieves this by applying a range of optimizations as well as making use of NVIDIA-specific hardware features \cite{nvidiainferencewhitepaper}. 
This integration with TensorRT, and its associated performance on NVIDIA products is the principal advantage of this runtime.

TensorRT-LLM provides a Python library similar to that of vLLM through which inference is performed.
Similarly to vLLM, TensorRT-LLM also implements pipeline and tensor parallelism as well as continuous batching \cite{nvidiainferencewhitepaper}.

% \subsection{Orca}
\subsection{llama.cpp}\label{subsection:llama.cpp}
% llama architecture
% llama features
% Custom CUDA kernels for running LLMs on NVIDIA GPUs (support for AMD GPUs via HIP and Moore Threads MTT GPUs via MUSA)

llama.cpp is a popular open-source runtime for LLM inference on CPUs, particularly designed to run models from the LLaMA family \cite{touvron2023llama}. 
It focuses on memory-efficient execution through quantization and the use of the ggml tensor library, which is tailored for CPU-based matrix operations \cite{ggml}.

The runtime is designed to operate entirely on CPUs, using ggml to build a computation graph and perform optimized tensor computations with minimal memory overhead. 
It employs mmap to load models directly into memory, reducing the initial memory load and improving scalability on devices with limited RAM. 
As stated earlier, llama.cpp supports the LLaMA family of models while also providing conversion tools to enable compatibility with other model formats, such as PyTorch checkpoints \cite{paszke2019pytorch}.

A key distinction between llama.cpp and other, more fully-featured runtimes like vLLM lies in their decoding strategies. 
llama.cpp employs a token-by-token decoding approach, generating one token at a time sequentially for each request. 
This minimizes memory usage and simplifies execution, making it suitable for resource-constrained environments but limiting overall throughput.
In contrast, other runtimes utilise batch-level decoding, processing multiple requests and generating tokens in parallel.
This approach leverages GPU parallelism to achieve significantly higher throughput, particularly for large-scale and concurrent workloads.
llama.cpp does not natively support distributed inference or multi-node serving but can be integrated with external orchestration tools \cite{ray} for these purposes.

\section{Optimisations}\label{section:optimisations}
\subsection{Sarathi-Serve}
\subsection{PodAttention}
\subsection{NanoFlow}
\subsection{Prism}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Ethical Issues} % 1-2 pages
The principal two ethical concerns of a project in this field relate to the potential for misuse as well as provenance issues surrounding the dataset on which the model was trained.

The potential for misuse of LLMs is vast, with many instances of LLM abuse already being documented.
LLMs have already been used to generate harmful or misleading content, such as phishing messages \cite{hazell2023spear} and large-scale misinformation on social media \cite{williams2024large}.
End users may be unable to distinguish between content created by a genuine user and content generated by an LLM and thus end up misinformed.

The large size of the datasets required to train these models create potential ethical and data protection issues.
Concerns exist regarding the ability for generative models to amplify existing biases in their training data, with some of these concerns borne out in cases like Microsoft's Tay chatbot \cite{tayMicrosoft}.
AI fairness is still an open area of research \cite{xivuri2021systematic} and it is unlikely that existing LLM models will be completely free of bias at inference time.
At the same time, the provenance of this training data is also an important ethical consideration.
Private or sensitive data has the potential to be incorporated into training sets and there exist cases \cite{nasr2023scalable} where this training data has then been generated verbatim at inference time, exposing this sensitive data to an end user.

This project aims to investigate potential bottlenecks and limitations of existing inference runtimes as they pertain to scaling test-time compute.
Eliminating some of these bottlenecks makes test-time compute scaling more feasible and enables users to improve the answer quality of off-the-shelf models without requiring an expensive training process.
While this represents a boon for the accessibility of this technology, with users no longer limited to a handful of offerings by large companies, it also increases the number of potentially malicious actors who are able to use LLMs.
Small and local deployments likely have less of the oversight that large LLM providers experience, and thus present greater risk.
These two elements must be carefully managed in order to adhere to reasonable ethical standards.

As a proof-of-concept rather than a production tool, this projectâ€™s impact is likely to be limited and gradual, allowing ethical concerns to be addressed as they emerge.

\chapter{Problem Setting and System Description}
\begin{comment}
  - I think this is where I explain the original search and learn benchmark
  - I should also detail all the optimisations done by marcel + otto for posterity (this also kind of makes it sound like I might have done some work there?)
  - I also need to list details of the model + config choices as well
\end{comment}
Here we describe in greater detail the system we use as a baseline for an implementation of test-time compute scaling. 
The original system we used came from an open-source project called `Search and Learn' \cite{beeching2024scalingtesttimecompute}.
This project implements several strategies for scaling test-time compute including beam search and best-of-n sampling. 
The `Search and Learn' project was primarily created to provide an open-source reference implementation of popular test-time compute scaling techniques that had been detailed in other studies \cite{snell2024scaling}.
As such, it is unconcerned with performance and lacks several easily-made modifications that would improve performance considerably.
These modifications are detailed below. 
We also list key parameter choices and settings before explaining and justifying our main evaluation metric.

\section{Search and Learn}
The `Search and Learn' project implements several popular test-time compute scaling techniques. 
These include best-of-n sampling, beam search and a novel Diverse Verifier Tree Search (DVTS) algorithm \cite{beeching2024scalingtesttimecompute}.
The aim of the project is to empirically demonstrate that different scaling techniques can be used depending on the problem difficulty and compute budget.
This is orthogonal to the goal of this project, which is to investigate bottlenecks in test-time scaling implementations, and so any optimisation we make or propose must not alter the output content of the system. 
Instead, we use `Search and Learn' as a reference point from which to begin our analysis and benchmarking.
We also use it as a starting point for our optimisation implementations.
For the purposes of our analysis, we only concerned ourselves with the beam search implementation, as this was shown to be the best search strategy on average \cite{beeching2024scalingtesttimecompute}.

\section{Problem Setting}
\subsection{Models}
The beam search implementation uses two transformer-based models.
The first, refered to as the `LLM', generates new reasoning steps in response to a user prompt. 
For our system, we use Meta's \texttt{Llama-3.2-1B-Instruct} model \cite{grattafiori2024llama}.
The second, refered to as the `PRM', produces a score for each reasoning step in the current LLM output. 
This is provided via the \texttt{Llama3.1-8B-PRM-Deepseek-Data} model \cite{dong2024rlhf} \cite{ArmoRM} \cite{xiong2024iterative}.
The LLM generates new reasoning steps as the beam search tree is explored. 
It requires a prefill stage to compute the attention states for all previous reasoning steps, but then uses the autoregressive decode stage to produce new chains of reasoning. 
As decode is limited by memory bandwidth rather than GPU FLOPs, we say this model is decode-bound. % is this true/the right way of explaining this?
The PRM scores a reasoning step by taking as input all of the LLM generated reasoning up to and including that step.
It emits a single output token and then the logits in the final layer for specific tokens are extracted and used as a score.
This is repeated for all reasoning steps.
The fact that the PRM only outputs a single token makes it heavily bound by the prefill stage of the inference pipeline.

The unique distinction between the LLM being decode-bound and the PRM being prefill-bound opens up opportunities for optimisation. % do i include this seeing as how my optimisations don't use this fact?

% diagram for this showing how prm creates a score for each step

\subsection{Dataset}
The original `Search and Learn' project uses a subset of 500 problems selected from the MATH dataset, a selection of problems taken from high-school maths competitions \cite{hendrycks2021measuring}.
These problems are assigned difficulty levels from one to five.
For our benchmarking, we use a set of 128 problems sampled uniformly across these difficulty levels.

\subsection{Beam Search Parameters}
To ensure our accuracy on the MATH dataset remains comparable with the original `Search and Learn` project for the purpose of evaluation, we keep our beam search parameters the same.
Namely we use a beam width of four and set a maximum tree depth of 40.
% Maybe this bit is weak - could take out since they can always look at the search and learn benchmark
% actually could always just add these two sentences to the evaluation metric section

\section{Modifications}
To ensure a reasonable baseline from which to evaluate performance, several changes were made to the `Search and Learn` reference code.
These represent simple optimisations to correct obvious performance deficiencies in the original codebase and were implemented by several members of Imperial College London's Large-Scale Data and Systems Group.

\subsection{Consistent Runtime} 
The original `Search and Learn` code uses two different runtimes to run the LLM and PRM, vLLM and Hugging Face's \texttt{transformers} library \cite{kwon2023efficient} \cite{wolf2019huggingface} respectively.
The decision was made to use the same runtime for the two models in order to provide a consistent runtime as a starting point for analysis.
vLLM is a library specifically optimized for inference performance, whereas the inference functionality in \texttt{transformers} is more of an add-on, primarily intended to demonstrate the range of models it supports.
As such, vLLM was chosen over \texttt{transformers}.
One significant impact of this is that it means the PRM benefits from vLLM's built in prefix caching, which is not a feature of the \texttt{transformers} library.

\subsection{Score Cache}
For each node in the beam search tree, the PRM needs to run to score the associated reasoning step.
This represents repeated computation, as the PRM score for a given reasoning step will remain the same regardless of how deeply the search tree is explored.
A score cache was implemented that holds the associated scores for previous PRM invocations.

\subsection{Continuous Problem Batching}
One major improvement in throughput comes from the implementation of continuous problem batching.
The `Search and Learn' implementation groups the MATH dataset questions into batches and feeds these into the beam search implementation, only running beam search on one batch at a time.
Owing to the varying difficulty of problems in this dataset, problems terminate their beam search at different times, meaning that the size of the input batches to both the LLM and PRM decrease as the search progresses, leading to underutilisation of the GPU.
To remedy this, a `minimum problem batch size' is introduced.
If the size of the input batch to the LLM or PRM is less than this minimum, problems from other batches are fed in to ensure that the GPU is fully utilised - this dramatically improves system throughput.

\subsection{End of Sequence Sampling}
The `Search and Learn' implementation imposes a maximum tree depth for the beam search.
When this depth is reached, instead of terminating the response, the implementation instead continues to sample from the beam until an end of sequence token is produced - we remove this feature in our implementation.
Empirically, sampling in this manner does little to improve the accuracy of the system on the MATH dataset.
If a problem has reached the maximum tree depth without terminating, the current chain of reasoning has likely diverged significantly from the correct answer and extra sampling is unlikely to improve the quality of the response.

End of sequence sampling dramatically changes the execution profile of the system, with a long execution period for both the LLM and PRM following the short, regular switching between models in earlier steps.
This more closely resembles the execution profile of a naive, non-test time scaling implementation. % bit weak, fix
As such, when we hit our maximum tree depth, we instead just output the response generated by the beam up to that point. 

\section{Evaluation Metrics}
We choose to focus our evaluation on the throughput of the test-time scaling system, rather than on latency.
This is motivated by the nature of the prompts that users typically submit to such systems - these are typically difficult problems involving long chains of reasoning.
As a result, users typically anticipate slower response times. 
In this setting, overall throughput, measured in tokens or prompts processed per unit time, better captures the performance bottlenecks and scalability of the system than per-query latency.

To validate that our optimisations preserve functional correctness, we consider the accuracy of the system on the MATH dataset. 
We run the original `Search and Learn` system on the MATH dataset to establish an accuracy baseline and rerun our optimised system on the same dataset to test that this accuracy has been maintained.
If it has, we can conclude with some degree of confidence that our optimisations have not altered the core behaviour of the system as it pertains to output content.
Notably, we do not compare outputs directly between systems, as the inherent non-determinism in vLLM, due to factors such as sampling temperature, makes meaningful per-token comparisons impossible.

\chapter{Investigations}
\begin{comment}
  - I think this should kind of be structured as a series of questions and answers where I sort of go "here's a hypothesis, here is how I tested it and these are the results".

  - Maybe start by detailing broad questions and then how I drilled down into answering them?

  - Questions I sought to investigate
    - What factors influence kv cache hit rate?
      - How does eviction policy impact performance?
    - How does changing the memory split between llm and prm impact performance
    - What are the memory requirements for both the llm and prm
      - This is the work I did regarding memory breakdown
\end{comment}

This project seeks to understand what throughput improvements to a beam search system are possible by optimising memory footprint.
This encompasses both inter-model memory considerations, like the split of memory between models, as well as intra-model considerations, like how a model's KV cache is managed.
To that end, we sought to answer the following questions through a series of experiments and investigations:
\begin{itemize}
    \item \textbf{What are the components of the LLM and PRM's memory requirements?}
    \item \textbf{How does changing the GPU memory split between LLM and PRM impact performance?}
    \item \textbf{What factors influence the KV cache hit rate?}
\end{itemize}
These investigations, and their relation to our proposed optimisation, are detailed below.

\section{Model Memory Requirements}
\begin{comment}
  - Need to understand memory breakdown in order to better understand how it is allocated/what can be moved around
  - do this by hooking into vLLM's memory allocation code
    - explain how this works
      - runs a dry run with maximum number of tokens
      - sees peak allocated memory by torch, as well as non-torch allocated memory
      - breaks this down into model weights, activations (we assume), non-torch allocated memory (nccl communicators etc. )
      - the rest is allocated as kv cache memory
    - Speak about how number of available gpu blocks available is affected by number of kv heads per layers and the number of total layers
  - as we can see, varying the utilisation parameter only changes the amount of memory allocated for the kv cache 
\end{comment}
We initially seek to better understand the memory footprint of a given model.
By understanding this in greater detail, we can more effectively design strategies to use memory optimally.

To do this, we hook into the code that vLLM uses to determine the size of a model's KV cache.
This process involves running a dry inference pass with the maximum number of tokens the model is configured to handle in a single request.
After this run, vLLM records the peak memory allocated by PyTorch
It also records any memory allocations that occur outside of PyTorch - these may arise as result of libraries like NCCL using GPU memory for communication primitives.
This total memory usage is then broken down into several components and used to determine the size of the KV cache.

Firstly, there is memory used for model weights - this is static and a function of the model used.
After this, we infer the memory used for intermediate activations during the dry run - this is not explicitly reported but can be assumed based on the remaining PyTorch-allocated memory left after accounting for model weights.
Non-PyTorch memory allocations, such as those for NCCL communicators and other runtime overheads, are accounted for separately by subtracting the peak GPU memory usage as measured by CUDA from the peak GPU memory used by PyTorch.
The memory remaining after these components are accounted for is allocated for vLLM's KV cache blocks.

\begin{equation*}
  \text{Mem}_{\text{KV}} = \text{Mem}_{\text{Total}} \times \text{Utilisation} - (\text{Mem}_{\text{Activations}} + \text{Mem}_{\text{Weights}} + \text{Mem}_{\text{NonTorch}})
\end{equation*}
As a result, when we vary vLLMâ€™s \texttt{gpu\_model\_utilization} parameter, which controls what fraction of total available memory is used for the KV cache, we observe that only the memory allocated for the KV cache changes, with the allocations for weights, activations, and non-PyTorch components remaining fixed.
This is best observed in figure %% add figure \ref{figure:memory-breakdown} 
From this, we can confirm that the \texttt{gpu\_model\_utilization} parameter exclusively affects how much space is reserved for the KV cache.

\section{Memory Split}
\begin{comment}
  - i think it would be cool if i could explain where the 0.3 thing comes from in terms of theory/number of blocks
  - speak about how block size varies
    - what determines block size
\end{comment}

Importantly, the number of available GPU memory blocks for the KV cache is influenced by model architecture, specifically the number of KV heads per layer and the total number of layers. 
These determine how much memory is required per token to store the KV cache.

\begin{equation*}
  \text{NumKVBlocks} = \frac{\text{Mem}_{\text{KV}}}{\text{PageSize} \times \text{NumLayers}}
\end{equation*}

We can observe the effect of varying 




\section{Batch Size}
\begin{comment}
  - 
\end{comment}

\section{KV Cache Eviction Policy}
\begin{comment}
  - explain vllm kv cache eviction policy
  - supporting plot
\end{comment}

\chapter{Optimisation}
\begin{comment}
  - Detail how my optimisation works and how it is implemented
\end{comment}

\section{Motivation}
\section{Design}
\section{Implementation}

\chapter{Evaluation}
\begin{comment}
  - Evaluate optimisation...
\end{comment}

\chapter{Conclusion and Future Work}
\begin{comment}
  - Keep this short and just summarise key findings and elaborate on future optimisations/research directions.
\end{comment}


%% bibliography
\bibliographystyle{vancouver}
\bibliography{references}


\end{document}
