@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec},
  year={2018}
}
@article{dai2015semi,
  title={Semi-supervised sequence learning},
  author={Dai, Andrew M and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}
@inproceedings{kenton2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={Proceedings of naacL-HLT},
  volume={1},
  pages={2},
  year={2019},
  organization={Minneapolis, Minnesota}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{sennrich2015neural,
  title={Neural machine translation of rare words with subword units},
  author={Sennrich, Rico},
  journal={arXiv preprint arXiv:1508.07909},
  year={2015}
}
@article{shi2024keep,
  title={Keep the Cost Down: A Review on Methods to Optimize LLM's KV-Cache Consumption},
  author={Shi, Luohe and Zhang, Hongyi and Yao, Yao and Li, Zuchao and Zhao, Hai},
  journal={arXiv preprint arXiv:2407.18003},
  year={2024}
}
@article{pope2023efficiently,
  title={Efficiently scaling transformer inference},
  author={Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  pages={606--624},
  year={2023}
}
@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}
@misc{cublas,
  title={Nvidia cuBLAS},
  howpublished = {\url{https://developer.nvidia.com/cublas}},
  note = {Accessed: 2025-01-21}
}
@article{choquette2021nvidia,
  title={Nvidia a100 tensor core gpu: Performance and innovation},
  author={Choquette, Jack and Gandhi, Wishwesh and Giroux, Olivier and Stam, Nick and Krashinsky, Ronny},
  journal={IEEE Micro},
  volume={41},
  number={2},
  pages={29--35},
  year={2021},
  publisher={IEEE}
}
@article{nickolls2008scalable,
  title={Scalable parallel programming with cuda: Is cuda the parallel programming model that application developers have been waiting for?},
  author={Nickolls, John and Buck, Ian and Garland, Michael and Skadron, Kevin},
  journal={Queue},
  volume={6},
  number={2},
  pages={40--53},
  year={2008},
  publisher={ACM New York, NY, USA}
}
@article{li2024large,
  title={Large language model inference acceleration: A comprehensive hardware perspective},
  author={Li, Jinhao and Xu, Jiaming and Huang, Shan and Chen, Yonghua and Li, Wen and Liu, Jun and Lian, Yaoxiu and Pan, Jiayi and Ding, Li and Zhou, Hao and others},
  journal={arXiv preprint arXiv:2410.04466},
  year={2024}
}
@inproceedings{yu2022orca,
  title={Orca: A distributed serving system for $\{$Transformer-Based$\}$ generative models},
  author={Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
  booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages={521--538},
  year={2022}
}
@article{hazell2023spear,
  title={Spear phishing with large language models},
  author={Hazell, Julian},
  journal={arXiv preprint arXiv:2305.06972},
  year={2023}
}
@article{williams2024large,
  title={Large language models can consistently generate high-quality content for election disinformation operations},
  author={Williams, Angus R and Burke-Moore, Liam and Chan, Ryan Sze-Yin and Enock, Florence E and Nanni, Federico and Sippy, Tvesha and Chung, Yi-Ling and Gabasova, Evelina and Hackenburg, Kobi and Bright, Jonathan},
  journal={arXiv preprint arXiv:2408.06731},
  year={2024}
}
@inproceedings{xivuri2021systematic,
  title={A systematic review of fairness in artificial intelligence algorithms},
  author={Xivuri, Khensani and Twinomurinzi, Hossana},
  booktitle={Responsible AI and Analytics for an Ethical and Inclusive Digitized Society: 20th IFIP WG 6.11 Conference on e-Business, e-Services and e-Society, I3E 2021, Galway, Ireland, September 1--3, 2021, Proceedings 20},
  pages={271--284},
  year={2021},
  organization={Springer}
}
@misc{tayMicrosoft,
  title={Learning from Tay's introduction},
  author={Lee, Peter},
  howpublished = {\url{https://web.archive.org/web/20241127051442/https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/}},
  note={Accessed: 2025-01-14},
  year={2016}
}
@article{nasr2023scalable,
  title={Scalable extraction of training data from (production) language models},
  author={Nasr, Milad and Carlini, Nicholas and Hayase, Jonathan and Jagielski, Matthew and Cooper, A Feder and Ippolito, Daphne and Choquette-Choo, Christopher A and Wallace, Eric and Tram{\`e}r, Florian and Lee, Katherine},
  journal={arXiv preprint arXiv:2311.17035},
  year={2023}
}
