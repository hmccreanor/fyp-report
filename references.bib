@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec},
  year={2018}
}
@article{dai2015semi,
  title={Semi-supervised sequence learning},
  author={Dai, Andrew M and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}
@inproceedings{kenton2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={Proceedings of naacL-HLT},
  volume={1},
  pages={2},
  year={2019},
  organization={Minneapolis, Minnesota}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{sennrich2015neural,
  title={Neural machine translation of rare words with subword units},
  author={Sennrich, Rico},
  journal={arXiv preprint arXiv:1508.07909},
  year={2015}
}
@article{hazell2023spear,
  title={Spear phishing with large language models},
  author={Hazell, Julian},
  journal={arXiv preprint arXiv:2305.06972},
  year={2023}
}
@article{williams2024large,
  title={Large language models can consistently generate high-quality content for election disinformation operations},
  author={Williams, Angus R and Burke-Moore, Liam and Chan, Ryan Sze-Yin and Enock, Florence E and Nanni, Federico and Sippy, Tvesha and Chung, Yi-Ling and Gabasova, Evelina and Hackenburg, Kobi and Bright, Jonathan},
  journal={arXiv preprint arXiv:2408.06731},
  year={2024}
}
@inproceedings{xivuri2021systematic,
  title={A systematic review of fairness in artificial intelligence algorithms},
  author={Xivuri, Khensani and Twinomurinzi, Hossana},
  booktitle={Responsible AI and Analytics for an Ethical and Inclusive Digitized Society: 20th IFIP WG 6.11 Conference on e-Business, e-Services and e-Society, I3E 2021, Galway, Ireland, September 1--3, 2021, Proceedings 20},
  pages={271--284},
  year={2021},
  organization={Springer}
}
@misc{tayMicrosoft,
  title={Learning from Tay's introduction},
  author={Lee, Peter},
  howpublished = {\url{https://web.archive.org/web/20241127051442/https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/}},
  note={Accessed: 2025-01-14},
  year={2016}
}
@article{nasr2023scalable,
  title={Scalable extraction of training data from (production) language models},
  author={Nasr, Milad and Carlini, Nicholas and Hayase, Jonathan and Jagielski, Matthew and Cooper, A Feder and Ippolito, Daphne and Choquette-Choo, Christopher A and Wallace, Eric and Tram{\`e}r, Florian and Lee, Katherine},
  journal={arXiv preprint arXiv:2311.17035},
  year={2023}
}
